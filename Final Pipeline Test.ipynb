{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3cde4bc6-645d-4246-b69a-97c0267f81a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be886587-916f-45c2-8732-b237afe671a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#basic imports\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, percent_rank, monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "#for EDA/plots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#for feature creation\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, OneHotEncoder, ChiSqSelector, Bucketizer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import concat, substring, lit, udf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Window as W\n",
    "\n",
    "#for modeling\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, NaiveBayes\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import itertools\n",
    "#for evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sparkdl.xgboost import XgboostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f53c8e6d-8d5a-49e5-9ad1-4f9d48df8fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "blob_container = \"team20fp\" # The name of your container created in https://portal.azure.com\n",
    "storage_account = \"w261fp\" # The name of your Storage account created in https://portal.azure.com\n",
    "secret_scope = \"team20scope\" # The name of the scope created in your local computer using the Databricks CLI\n",
    "secret_key = \"team20key\" # The name of the secret key created in your local computer using the Databricks CLI \n",
    "blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "mount_path = \"/mnt/mids-w261\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e30fb3e8-3df0-406b-8aa0-fbf11015655d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/airlines_weather_full/</td><td>airlines_weather_full/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/airport_gps.csv</td><td>airport_gps.csv</td><td>760437</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/airport_utc.csv</td><td>airport_utc.csv</td><td>1127225</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/airports_weatherStations_joined/</td><td>airports_weatherStations_joined/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/df_carrier_delayed_flights_vol/</td><td>df_carrier_delayed_flights_vol/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/df_carrier_flights_vol/</td><td>df_carrier_flights_vol/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/df_flight_delay_proportion/</td><td>df_flight_delay_proportion/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/df_flight_delay_proportion2/</td><td>df_flight_delay_proportion2/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/df_planes_by_carrier/</td><td>df_planes_by_carrier/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/feature_set_full_sorted/</td><td>feature_set_full_sorted/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/final_sets/</td><td>final_sets/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/folds5_full_features/</td><td>folds5_full_features/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/folds5_lim_features/</td><td>folds5_lim_features/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_1/</td><td>val_pred_xgb_1/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_2/</td><td>val_pred_xgb_2/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_3/</td><td>val_pred_xgb_3/</td><td>0</td></tr><tr><td>wasbs://team20fp@w261fp.blob.core.windows.net/val_set_with_predprob/</td><td>val_set_with_predprob/</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/airlines_weather_full/",
         "airlines_weather_full/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/airport_gps.csv",
         "airport_gps.csv",
         760437
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/airport_utc.csv",
         "airport_utc.csv",
         1127225
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/airports_weatherStations_joined/",
         "airports_weatherStations_joined/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/df_carrier_delayed_flights_vol/",
         "df_carrier_delayed_flights_vol/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/df_carrier_flights_vol/",
         "df_carrier_flights_vol/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/df_flight_delay_proportion/",
         "df_flight_delay_proportion/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/df_flight_delay_proportion2/",
         "df_flight_delay_proportion2/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/df_planes_by_carrier/",
         "df_planes_by_carrier/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/feature_set_full_sorted/",
         "feature_set_full_sorted/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/final_sets/",
         "final_sets/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/folds5_full_features/",
         "folds5_full_features/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/folds5_lim_features/",
         "folds5_lim_features/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_1/",
         "val_pred_xgb_1/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_2/",
         "val_pred_xgb_2/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/val_pred_xgb_3/",
         "val_pred_xgb_3/",
         0
        ],
        [
         "wasbs://team20fp@w261fp.blob.core.windows.net/val_set_with_predprob/",
         "val_set_with_predprob/",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(blob_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42d39c31-c92b-4fba-a5cc-462126ed480c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import Data\n",
    "\n",
    "For test on 2019 data, uncomment the last two lines in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "94d51bdb-76d8-4912-8399-fa4667d7df93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = spark.read.parquet(f\"{blob_url}/final_sets/train\").withColumnRenamed('DEP_DEL15', 'label')\n",
    "df_test = spark.read.parquet(f\"{blob_url}/final_sets/test\").withColumnRenamed('DEP_DEL15', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eae58702-3af3-4c38-b52c-718e62834980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[6]: 19</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[6]: 19</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.select('OP_CARRIER').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6437747-9f33-44c4-a5c5-9eba259dd327",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modeling Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0915a44-1d65-4b4c-87eb-bdc32a18cdc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################# functions for prepping data ###################\n",
    "\n",
    "def balance_trainset(train_df, up_or_down = 'up'):\n",
    "    ''' Balance classes in training dataset'''\n",
    "    \n",
    "    num_delay = train_df.filter(F.col('label') == 1).count()\n",
    "    num_ontime = train_df.filter(F.col('label') == 0).count()\n",
    "    \n",
    "    #total_obs = num_delay + num_ontime\n",
    "    \n",
    "    if up_or_down == 'up':\n",
    "        pct = num_ontime/num_delay\n",
    "        \n",
    "        ontime_df = train_df.filter(F.col('label') == 0)\n",
    "        delay_df = train_df.filter(F.col('label') == 1).sample(withReplacement = True, fraction = pct, seed = 1)\n",
    "        \n",
    "    \n",
    "    elif up_or_down == 'down':\n",
    "        pct = num_delay/num_ontime\n",
    "        \n",
    "        delay_df = train_df.filter(F.col('label') == 1)\n",
    "        ontime_df = train_df.filter(F.col('label') == 0).sample(withReplacement = False, fraction = pct, seed = 1)\n",
    "    \n",
    "    \n",
    "    train_balance = delay_df.union(ontime_df)\n",
    "    print(f'Balancing factor: {round(pct,2)}')\n",
    "    print(f'Num observations after balancing: {train_balance.count()}')\n",
    "    return train_balance\n",
    "\n",
    "################### functions for modeling ######################\n",
    "\n",
    "def lr_pipeline(model, cts_features, cat_features, bucket_features):\n",
    "    ''' Pipeline to scale continuous features and encode categorical features'''\n",
    "    \n",
    "    #scale continuous features\n",
    "    vect_cts = VectorAssembler(inputCols = cts_features,\n",
    "                               outputCol = 'cts_feats',\n",
    "                              handleInvalid = 'skip')\n",
    "    scaler = StandardScaler(inputCol = 'cts_feats',\n",
    "                            outputCol = 'scaled_cts_feats',\n",
    "                            withStd = True, withMean = True)\n",
    "    \n",
    "    ##test bucketed vars\n",
    "    vect_bucket = Bucketizer(inputCols = bucket_features,\n",
    "                                         splitsArray = [[0,10,25,50,100,250,500,1000,float('inf')], #orig_fpd\n",
    "                                           [0,10,25,50,100,250,500,1000,float('inf')], #dest_fpd\n",
    "                                           [-float('inf'), 4023, float('inf')], #vis\n",
    "                                           [-float('inf'), -156, float('inf')], #air_tmp\n",
    "                                           [1,2,3,4,5,7,float('inf')], #distance (group)\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct carrier del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct route del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct orig del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')] #pct dest del\n",
    "                                          ],\n",
    "                            outputCols = [col+'_bucket' for col in bucket_features])\n",
    "    \n",
    "    #index string values before one hot encoding (also works on numeric categoricals, will convert to string then index)\n",
    "    indexed = StringIndexer(inputCols = cat_features+[col+'_bucket' for col in bucket_features],\n",
    "                            outputCols = [col+'_idx' for col in cat_features+bucket_features],\n",
    "                            handleInvalid = 'keep')\n",
    "    \n",
    "    \n",
    "    onehot_feats = OneHotEncoder(inputCols = [col+'_idx' for col in cat_features+bucket_features],\n",
    "                                outputCols = [col+'_enc' for col in cat_features+bucket_features])\n",
    "    \n",
    "    vect_cat = VectorAssembler(inputCols = [col+'_enc' for col in cat_features+bucket_features],\n",
    "                              outputCol = 'cat_feats')\n",
    "    \n",
    "\n",
    "    #combine cts and cat features\n",
    "    combined_vect = VectorAssembler(inputCols = ['scaled_cts_feats', 'cat_feats'],\n",
    "                                   outputCol = 'features')\n",
    "    \n",
    "    #combine pipeline components\n",
    "    pipeline = Pipeline(stages = [vect_cts, scaler, vect_bucket, indexed, onehot_feats, vect_cat, combined_vect, model])\n",
    "    return pipeline\n",
    "\n",
    "# tree algo pipeline - no scaler, no feature selector\n",
    "def tree_pipeline(model, cts_features, cat_features, bucket_features):\n",
    "    \n",
    "    #cts vars\n",
    "    vect_cts = VectorAssembler(inputCols = cts_features,\n",
    "                               outputCol = 'cts_feats',\n",
    "                              handleInvalid = 'skip')\n",
    "    \n",
    "    #cts vars to bucket\n",
    "    vect_bucket = Bucketizer(inputCols = bucket_features,\n",
    "                            splitsArray = [[0,10,25,50,100,250,500,1000,float('inf')], #orig_fpd\n",
    "                                           [0,10,25,50,100,250,500,1000,float('inf')], #dest_fpd\n",
    "                                           [-float('inf'), 4023, float('inf')], #vis\n",
    "                                           [-float('inf'), -156, float('inf')], #air_tmp\n",
    "                                           [1,2,3,4,5,7,float('inf')], #distance (group)\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct carrier del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct route del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')], #pct orig del\n",
    "                                           [0, 0.20, 0.35, 0.55, 0.75, float('inf')] #pct dest del\n",
    "                                          ],\n",
    "                             outputCols = [col+'_bucket' for col in bucket_features])\n",
    "    \n",
    "    #cat vars\n",
    "    indexed = StringIndexer(inputCols = cat_features+[col+'_bucket' for col in bucket_features],\n",
    "                            outputCols = [col+'_idx' for col in cat_features+bucket_features],\n",
    "                            handleInvalid = 'keep')\n",
    "    \n",
    "    \n",
    "    onehot_feats = OneHotEncoder(inputCols = [col+'_idx' for col in cat_features+bucket_features],\n",
    "                                outputCols = [col+'_enc' for col in cat_features+bucket_features])\n",
    "    \n",
    "    vect_cat = VectorAssembler(inputCols = [col+'_enc' for col in cat_features+bucket_features],\n",
    "                              outputCol = 'cat_feats')\n",
    "    \n",
    "    combined_vect = VectorAssembler(inputCols = ['cts_feats', 'cat_feats'],\n",
    "                                   outputCol = 'features')\n",
    "    \n",
    "    #combine pipeline components\n",
    "    pipeline = Pipeline(stages = [vect_cts, vect_bucket, indexed, onehot_feats, vect_cat, combined_vect, model])\n",
    "    return pipeline\n",
    "\n",
    "    \n",
    "def gen_model_pipeline(model_type, param_dict, cts_features, cat_features, bucket_features = None):\n",
    "    '''Input model type and parameters, return model pipeline'''\n",
    "    \n",
    "    #still need to create dict with parameters for each model\n",
    "    params = param_dict[model_type]\n",
    "    if model_type == 'lr':\n",
    "        lr = LogisticRegression(regParam = params['regParam']\n",
    "                                #,weightCol = 'label_weight'\n",
    "                               )\n",
    "        pipeline = lr_pipeline(lr, cts_features, cat_features, bucket_features)\n",
    "    \n",
    "    elif model_type == 'gbt':\n",
    "        gbt = GBTClassifier(maxDepth = params['maxDepth'],\n",
    "                           maxBins = params['maxBins'],\n",
    "                           maxIter = params['maxIter'],\n",
    "                           stepSize = params['stepSize'])\n",
    "        pipeline = tree_pipeline(gbt, cts_features, cat_features, bucket_features)\n",
    "    \n",
    "    elif model_type == 'xgb':\n",
    "\n",
    "        xgb = XgboostClassifier(labelCol = 'label',\n",
    "                                featuresCol = 'features',\n",
    "                                missing = 0.0,\n",
    "                                rawPredictionCol = 'probability',\n",
    "                                #booster = params['booster'], #defaults to gbtree\n",
    "                                max_depth = params['max_depth'],\n",
    "                                n_estimators = params['n_estimators'],\n",
    "                                reg_lambda = params['reg_lambda'],\n",
    "                                reg_alpha = params['reg_alpha'],\n",
    "                                objective = params['objective'],\n",
    "                                base_score = params['base_score'],\n",
    "                                gamma = params['gamma'],\n",
    "                                scale_pos_weight = params['scale_pos_weight'],\n",
    "                                min_child_weight = params['min_child_weight'],\n",
    "                                #max_delta_step = params['max_delta_step'],\n",
    "                                learning_rate = params['learning_rate'],\n",
    "                                max_bin = params['max_bin']\n",
    "                               )\n",
    "        \n",
    "        pipeline = tree_pipeline(xgb, cts_features, cat_features, bucket_features)\n",
    "        \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_model(train_df, model_type, param_dict, cts_features, cat_features, bucket_features = None, balance_type = 'up', pipeline = None):\n",
    "    ''' Balance train_df, generate model pipeline using best params, train model'''\n",
    "    \n",
    "    #balance train_df\n",
    "    if balance_type == 'up':\n",
    "        train_df = balance_trainset(train_df, up_or_down = 'up').persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    elif balance_type == 'down':\n",
    "        train_df = balance_trainset(train_df, up_or_down = 'down').persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    elif balance_type == 'weight':\n",
    "        train_df = weight_classes(train_df).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    #elif balance_type == None:\n",
    "        #train_df = train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    #print('Train dataset balancing complete')  \n",
    "    \n",
    "    #gen model pipeline using best params (need to find thru CV)\n",
    "    #params = param_dict[model_type]\n",
    "    if pipeline == None:\n",
    "        pipeline = gen_model_pipeline(model_type, param_dict, cts_features, cat_features, bucket_features)\n",
    "        #print('Pipeline generation complete')\n",
    "    #train model\n",
    "    model = pipeline.fit(train_df)\n",
    "    #print('Training complete')\n",
    "    train_df.unpersist()\n",
    "    return model\n",
    "\n",
    "\n",
    "################# functions for evaluation ######################\n",
    "\n",
    "def eval_p_r_f2(df, acc = True):\n",
    "    pred_rdd = df.select(['prediction', 'label']).rdd\n",
    "    multi_metrics = MulticlassMetrics(pred_rdd)\n",
    "    precision = multi_metrics.precision(1)\n",
    "    recall = multi_metrics.recall(label = 1)\n",
    "    f2 = multi_metrics.fMeasure(1.0,2.0)\n",
    "    if acc:\n",
    "        score =  multi_metrics.accuracy\n",
    "        return(precision, recall, f2, score)\n",
    "    return (precision, recall, f2)\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d0bf2e9e-09c9-420b-a9fd-5a2fe64c0f1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0aaf544-2a2f-482e-a33c-856176a60898",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cts_vars = ['PRE_FL_WINDOW','CUMAVG_WND_DIR_WEEKLY','CUMAVG_DEW_WEEKLY','CUMAVG_VIS_WEEKLY','CUMAVG_SLP_WEEKLY','CUMAVG_WND_SPEED_WEEKLY','CUMAVG_CEIL_HEIGHT_WEEKLY','CUMAVG_AIR_TMP_WEEKLY','CUMAVG_DEP_DELAY_WEEKLY','CUMAVG_DEP_DEL15_WEEKLY','CUMAVG_ARR_DELAY_WEEKLY','CUMAVG_ARR_DEL15_WEEKLY']\n",
    "\n",
    "cat_vars = ['ORIGIN','YEAR','QUARTER','DEST','MONTH','DAY_OF_WEEK','OP_CARRIER','PRIOR_ARR_DEL','PRIOR_DEP_DEL','ORIG_DEST']\n",
    "\n",
    "bucket_vars = ['ORIG_FPD','DEST_FPD','VIS','AIR_TMP','DISTANCE_GROUP','PCT_CARRIER_DEL','PCT_ROUTE_DEL','PCT_ORIG_DEL','PCT_DEST_DEL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ebf259fb-3a6e-4791-9f7a-620f71f7cf5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1bc67543-93cd-4ad4-bcea-70a385789d66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Balancing factor: 0.22\n",
       "Num observations after balancing: 8303507\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Balancing factor: 0.22\nNum observations after balancing: 8303507\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'xgb':{'max_depth':6,\n",
    "                  'n_estimators':150,\n",
    "                  'reg_lambda':1,\n",
    "                  'reg_alpha':0.2,\n",
    "                  'tree_method':'hist',\n",
    "                  'objective':'binary:logistic',\n",
    "                  'base_score':0.5,\n",
    "                  'gamma':0.05,\n",
    "                  'min_child_weight':1.5,\n",
    "                  'max_bin': 50,\n",
    "                  'learning_rate' : 0.2}}\n",
    "\n",
    "xgb_model_2 = fit_model(df_train,\n",
    "                       'xgb',\n",
    "                       params,\n",
    "                       cts_features = cts_vars,\n",
    "                       cat_features = cat_vars,\n",
    "                       bucket_features = bucket_vars,\n",
    "                       balance_type = 'down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "995f03dc-936e-429e-a845-7cdd8caee1c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "065d4482-49b6-4031-be39-155a38585faf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_pred2 =xgb_model_2.transform(df_test)\n",
    "precision_2, recall_2, f2_2, score_2 = eval_p_r_f2(val_pred2, acc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c8bf03b-4097-4b35-a4ca-587f25b73c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Precision: 0.3286082091967165\n",
       "Recall: 0.7575723665447379\n",
       "F2: 0.6007332855798426\n",
       "Accuracy: 0.6637104519178147\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Precision: 0.3286082091967165\nRecall: 0.7575723665447379\nF2: 0.6007332855798426\nAccuracy: 0.6637104519178147\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Precision: {precision_2}')\n",
    "print(f'Recall: {recall_2}')\n",
    "print(f'F2: {f2_2}')\n",
    "print(f'Accuracy: {score_2}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final Pipeline Test",
   "notebookOrigID": 1858507102387041,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
